
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Copy Text Buttons</title>
    <style>
        button {
            margin: 5px;
        }

        body {
            background-color: #f0f0f0;
            /* Light gray background */
            font-family: Arial, sans-serif;
            text-align: center;
        }

        button {
            margin: 10px;
            padding: 10px;
            font-size: 16px;
            background-color: #ddd;
            /* Light gray button background */
            border: 1px solid #aaa;
            /* Dark gray border */
            cursor: pointer;
        }

        button:hover {
            background-color: #ccc;
            /* Slightly darker background on hover */
        }
    </style>
</head>

<body>

    <button onclick="copyText(text1)">1a using Naive Bayes algorithm</button>
    <button onclick=" copyText(text2)">1b using the SVM classifier</button>
    <button onclick="copyText(text3)">2a clustering technique KMeans </button>
    <button onclick="copyText(text4)">2b  using hierarchical clustering</button>
    <button onclick="copyText(text5)">3 Perform linear regression</button>
    <button onclick="copyText(text6)">4 Perform logistic regression</button>
    <button onclick="copyText(text7)">5 Perform data analysis</button>
    
    
    <p id="copiedMsg"></p>
    <script>
        var text1 = `1a 

        import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

df = pd.read_csv('C:/Users/shett/Downloads/spam - spam.csv', encoding='latin-1')

df = df[['Message', 'Category']]
df.columns = ['SMS', 'Type']

countvec = CountVectorizer(ngram_range=(1, 4), stop_words='english', strip_accents='unicode', max_features=1000)

bow = countvec.fit_transform(df.SMS)

X_train = bow.toarray()
Y_train = df.Type.values

mnb = MultinomialNB()

mnb.fit(X_train, Y_train)

text1 = countvec.transform(['free gifts for all'])
print('free gifts for all')
print(mnb.predict(text1))
text2 = countvec.transform(['we will go for lunch'])
print('we will go for lunch')
print(mnb.predict(text2))

    `;
        var text2 = `1b

from sklearn import svm, datasets
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
iris = datasets.load_iris()
X = iris.data[:, :2] # we only take the first two features
y = iris.target
x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.3)
clf = svm.SVC(kernel='linear', C=1).fit(x_train, y_train)
classifier_predictions = clf.predict(x_test)
print(accuracy_score(y_test, classifier_predictions)*100)
h = 0.02
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
print(x_min)
print(x_max)
print(y_min)
print(y_max)
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))
print(xx.ravel())
print("---------------------")
print(yy.ravel())
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
print(Z)
plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.4)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("Linear SVM")
plt.show()

`;

        var text3 = `2a
        # importing libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# Importing the dataset
dataset = pd.read_csv(r'C:\Users\shett\Downloads\Mall_Customers - Mall_Customers.csv')
x = dataset.iloc[:, [3, 4]].values
#finding optimal number of clusters using the elbow method
#finding optimal number of clusters using the elbow method
from sklearn.cluster import KMeans
wcss_list= []  #Initializing the list for the values of WCSS
#Using for loop for iterations from 1 to 10.
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state= 42)
    kmeans.fit(x)
    wcss_list.append(kmeans.inertia_)
print("WCSS List:",wcss_list)
plt.plot(range(1, 11), wcss_list)
plt.title('The Elbow Method Graph')
plt.xlabel('Number of clusters(k)')
plt.ylabel('wcss_list')
plt.show()
#training the K-means model on a dataset
kmeans = KMeans(n_clusters=5, init='k-means++', random_state= 42)
y_predict= kmeans.fit_predict(x)
print(y_predict)
#visulaizing the clusters
plt.scatter(x[y_predict == 0, 0], x[y_predict == 0, 1], s=100, c='blue', label='Cluster 1')  # for first cluster
plt.scatter(x[y_predict == 1, 0], x[y_predict == 1, 1], s=100, c='green', label='Cluster 2')  # for second cluster
plt.scatter(x[y_predict == 2, 0], x[y_predict == 2, 1], s=100, c='red', label='Cluster 3')  # for third cluster
plt.scatter(x[y_predict == 3, 0], x[y_predict == 3, 1], s=100, c='cyan', label='Cluster 4')  # for fourth cluster
plt.scatter(x[y_predict == 4, 0], x[y_predict == 4, 1], s=100, c='magenta', label='Cluster 5')  # for fifth cluster
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label='Centroid')
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

`;

        var text4 = `2b
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
data = pd.read_csv(r'C:\Users\shett\Downloads\Wholesale customers data - Wholesale customers data.csv')
print("Original Data:")
print(data.head())
from sklearn.preprocessing import normalize
data_scaled = normalize(data)
data_scaled = pd.DataFrame(data_scaled, columns=data.columns)
print("\nNormalized Data:")
print(data_scaled.head())
import scipy.cluster.hierarchy as shc
plt.figure(figsize=(10, 7))
plt.title("Dendrograms")
linkage_matrix = shc.linkage(data_scaled, method='ward')
print("Linkage Matrix:", linkage_matrix)
dendrogram = shc.dendrogram(linkage_matrix)
plt.axhline(y=6, color='r', linestyle='--')
plt.show()
from sklearn.cluster import AgglomerativeClustering
cluster = AgglomerativeClustering(n_clusters=2, linkage='ward')
cluster_labels = cluster.fit_predict(data_scaled)
print("Cluster Labels:", cluster_labels)
plt.figure(figsize=(10, 7))
plt.scatter(data_scaled.iloc[:, 3], data_scaled.iloc[:, 4], c=cluster_labels, cmap='viridis') # Use appropriate column indices for Milk and Grocery
plt.title('Clusters using Agglomerative Hierarchical Clustering')
plt.xlabel('Milk')
plt.ylabel('Frozen')
plt.show()

        

`;

        var text5 = `3
        import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# Load the dataset
dataset = pd.read_csv(r'C:\Users\shett\Downloads\salary_data - salary_data.csv')
# Extract features (X) and target variable (y)
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
print(dataset.head())
# Split the dataset into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
# Train the linear regression model
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)
# Predictions on the test set
y_pred = regressor.predict(X_test)
print(pd.DataFrame(data={'Actuals':y_test,'Predictions':y_pred}))
# Plotting training set results
plt.scatter(X_train, y_train, color='red')
plt.plot(X_train, regressor.predict(X_train), color='blue')
plt.title('Salary vs Experience (Training set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()
# Plotting test set results
plt.scatter(X_test, y_test, color='red')
plt.plot(X_test, y_pred, color='blue')  # Use y_pred instead of regressor.predict(X_train)
plt.title('Salary vs Experience (Test set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()



        
  `;
  
        var text6 = `4
         import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
# Load the dataset
student_data = pd.read_csv(r"C:\Users\shett\Downloads\Admission_P1A - Admission_P1A.csv")
# Print first ten records
print(student_data.head(10))
# Split dataset into features and target variable
feature_cols = ['gre', 'gpa', 'rank']
X = student_data[feature_cols]
Y = student_data.admit
# 70% training and 30% test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
# Model building
clf = LogisticRegression()
# Fit the model with data
clf.fit(X_train, Y_train)
# Predict the response for test dataset
Y_pred = clf.predict(X_test)
# Accuracy calculation and display
print("Accuracy:", round(accuracy_score(Y_test, Y_pred),1))
# Prediction for new data
new = {'gre': [260], 'gpa': [2.67], 'rank': [1]}
sc2 = pd.DataFrame(new,columns=['gre','gpa','rank'])
# Predict admission for new data
Y_pred = clf.predict(sc2)
print(sc2)
print("Forecast is:", Y_pred)

`;

        var text7 = `5
# create and evaluate a static autoregressive model
from pandas import read_csv
from matplotlib import pyplot
from statsmodels.tsa.ar_model import AutoReg
from sklearn.metrics import mean_squared_error
from math import sqrt
# load dataset
series = read_csv(r'C:\Users\shett\Downloads\daily-min-temperatures.csv', header=0, index_col=0,parse_dates=True)
print(series.head(10))
print("========================================================================")
# split dataset
X = series.values
print(X)
print("========================================================================")
train, test = X[1:len(X)-7], X[len(X)-7:]
# train autoregression
model = AutoReg(train,10)
model_fit = model.fit()
print('Lag: %s' % model_fit.ar_lags)
print('Coefficients: %s' % model_fit.params)
# make predictions
predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)
for i in range(len(predictions)):
  print('predicted=%f, expected=%f' % (predictions[i], test[i]))
rmse = sqrt(mean_squared_error(test, predictions))
print('Test RMSE: %.3f' % rmse)
# plot results
pyplot.plot(test, label='Actual')
pyplot.plot(predictions, color='red',label='Predicted')
pyplot.legend()
pyplot.show()

`;


function copyText(text) {
    navigator.clipboard.writeText(text).then(function () {
        document.getElementById('copiedMsg').innerHTML = "Text Copied"
    }).catch(function (err) {
        console.error('Unable to copy text', err);
    });
}
</script>

</body>

</html>
